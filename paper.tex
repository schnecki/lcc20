\documentclass[envcountsame]{llncs}
\usepackage[OT1]{fontenc}
\usepackage[utf8x]{inputenc}

\usepackage{fullpage}
\usepackage[hidelinks]{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{listings}
\usepackage{array}
\usepackage{algorithm}
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
% \usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{pifont}
\usepackage{caption}
\usepackage{bbding}
\usepackage{comment}
\usepackage{url}
\usepackage{proof}
\usepackage{turnstile}

\usepackage{tikz}
\usetikzlibrary{patterns}
\usetikzlibrary{calc}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{shapes.geometric}
\usepackage{scalefnt}


\usepackage{paper}

\newcommand\MS[2][r]{\ifx t#1 \textcolor{blue}{[#2]}%
\else \begin{center}\textcolor{blue}{#2} \end{center} \fi}%
\newcommand{\Author}{Manuel Schneckenreither}
\newcommand{\Email}{manuel.schneckenreither@uibk.ac.at}
\newcommand{\Title}{Dynamic Strategies for \tct{}}
\hypersetup{
 pdfauthor={Manuel Schneckenreither, MSc MSc},
 pdftitle={Improving \tct{} with Amortised Resource Analyses and Dynamic Strategy Building},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={}, 
 pdflang={English}}
\date{\today}


\title{
  \Title
}
\author{\Author% \inst{1}
  % \href{https://orcid.org/0000-0000-0000-0000}{\aiOrcid \hspace{2mm} orcid.org/0000-0000-0000-0000}
  \orcidID{0000-0002-4812-4665}
}
% \authorrunning{Schneckenreither and Haeussler}
\institute{
  Department of Information Systems,
  Production and Logistics Management,
  University of Innsbruck, Austria\\
  email: \email{\Email}
}

\usepackage{etoolbox}
\newcommand{\zerodisplayskips}{%
  \setlength{\abovedisplayskip}{0pt}%
  \setlength{\belowdisplayskip}{0pt}%
  \setlength{\abovedisplayshortskip}{0pt}%
  \setlength{\belowdisplayshortskip}{0pt}}
\appto{\normalsize}{\zerodisplayskips}
\appto{\small}{\zerodisplayskips}
\appto{\footnotesize}{\zerodisplayskips}
\begin{document}

% \setlength{\abovedisplayskip}{0pt}
% \setlength{\belowdisplayskip}{0pt}
% \setlength{\abovedisplayshortskip}{0pt}
% \setlength{\belowdisplayshortskip}{0pt} 

\newcommand{\Abstract}{
  %
  Static program analysers are important pieces of software which sometimes are crucial and their
  failure or incapability of analysing input programs can lead to fatal failure, including the loss
  of human life. Unfortunately, static analyses are undecidable. Thus, some sort of creativity is
  needed when building fully automatic static code analysers. We propose to use reinforcement
  learning to introduce this required creativity in \tct{}, a complexity analyser for a generalised
  form of functional programs, namely term rewrite systems.
  %
  \keywords{\tct{}, static code analysis, time complexity, average reward adjusted reinforcement learning}
}

% \begin{abstract}
% \MS{TODO}
% \end{abstract}

% \mailaddress{manuel.schneckenreither@uibk.ac.at}
% \matriculationnumber{01117198}
% \supervisor{Assoc.~Prof.~Dr.~Georg Moser}

\clearpage\maketitle
% \tableofcontents

\abstract{\Abstract}


\section{Introduction}

Being able to perform space and runtime complexity analyses of programs is important for various
tasks, including but not limited to comparing algorithms, designing efficient programs or to
identify performance bottlenecks in software~\cite{Aho:1974:DAC:578775}. Some applications, as for
instance scheduling~\cite{puschner2000guest} are even safety crucial in real-time computer systems
and their failure can lead to a disaster, including the loss of human
life~\cite{engblom2000modeling}.
%
% Besides these obvious implications of program analysis there are other applications as well. For
% instance, in schedulability analyses of safety critical real-time computer systems worst-case
% execution times can be used to test whether a given set of tasks will meet the timing requirements
% of the application on a given target system and thus is schedulable or not~\cite{puschner2000guest}.
% This information can be used to ensure timely responses from interrupts which might be crucial and
% failure could lead to a disaster, sometimes including the loss of human
% life~\cite{engblom2000modeling}. Similarly, the applications of the analysis of lower bounds include
% performance debugging, program optimisation and verification. But, in the literature one also finds
% the automation of parallelisation as application area, cf.~\cite{DebrayLHL97,AlbertGM13}.

Usually we are interested in the runtime complexity of a program in regard to the programs' input
\textit{size}, e.g.\@ the length of the input list when analysing a sorting algorithm.
%
The time needed for the evaluation of an algorithm expressed as a function of its size is called the
\textit{time complexity}~\cite{Aho:1974:DAC:578775}. The \textit{asymptotic time complexity} is the
limiting behavior of the complexity as the problem's size increases. To be more precise, if a
problem of size \(n\) is processed by an algorithm in time \(c n^{x}\) for some constant \(c\), then
the algorithm is in the class \(\bO(n^{x})\). Analogous (asymptotic) space complexity is defined.
Futher, the \textit{worst-case complexity} is the maximum complexity over all inputs of a given
size~\cite{Aho:1974:DAC:578775}. In contrast, if for a given size the complexity is taken as the
minimum complexity over all inputs of a that size, the complexity is called \textit{best-case
  complexity}~\cite{Wegbreit75}.


% Most often we are concerned with time or space complexities of
% algorithms~\cite{Aho:1974:DAC:578775}. These complexities are measured in a quantity, called
% \textit{size}, of input data, which is usually represented as an integer value. For example, the
% size for an algorithm which reverses an input list could be the number of elements in the list.
% Similarly, for a graph problem the number of nodes could specify it's size.

% The time needed for the evaluation of an algorithm expressed as a function of it's size is called
% the \textit{time complexity}~\cite{Aho:1974:DAC:578775}. The \textit\{asymptotic time
% complexity\} is the limiting behavior of the complexity as the problem's size increases. To be more
% precise, if a problem of size \(n\) is processed by an algorithm in time \(c n^{x}\) for some constant
% \(c\), then the algorithm is in the class \(\bO(n^{x})\). Analogous (asymptotic) space complexity can be
% defined.


% Clearly, once the best-case and worst-case complexities of
% algorithms are known, the (asymptotic) time or space complexities are predetermined and algorithms
% can be compared to each other.

% This allows choosing of appropriate algorithms when designing
% applications. For instance, it might be crucial for security reasons to not provide location
% information of database entries to external users via the application interface. Thus, if a database
% request takes considerably more time once the needed information is located at the end of the
% database table as compared to the beginning, then the users could imply the location of the
% requested information according to the respond time. An algorithm with the same or similar
% (asymptotic) best-case and worst-case complexities could be chosen to prevent such an attack.

% Therefore, the aim is to find runtime complexities of programs. However, 
Clearly an evaluation of, e.g.\@ the runtime, of programs by using sample inputs and tracing the
actual runtime complexity is an unwise decision. This is due to (i) possibly infinitely many input
values, and (ii) as an extrapolation of the expected runtime is unsound. However, regarding the
applications it is of paramount importance to report sound complexities, which leads to the fact
that the only option are static program analyses. Unfortunately, at the same time it is known that
static analyses are undecidable~\cite{landi1992undecidability}, i.e.\@ it is not possible to build
an analyser which is able to infer the complexity for any given input program.
%
Despite this negative result \tct{}~\cite{avanzini2016tct} was developed to automatically infer
complexities of computer programs, whereas the input to \tct{} are (first as well as higher-order)
functional programs in their generalised form of term rewrite systems (TRS). The analysis of \tct{}
is not restricted to program-like applications, but allows standard term rewrite systems to be
analysed, regardless of confluence, left-linearity, or other restrictions~\cite{avanzini2016tct}. In
case the analysis succeeds \tct{} outputs a certification proofing the presented asymptotic
complexity of the input program. \tct{} is a built modular in the sense that it decomposes the input
program into a number of subprograms and applies its various methods on these, including further
decomposition steps if required. 

% and fully automatic complexity analyser for term rewrite systems (TRS) \cite{avanzini2016tct}.
% this on one hand does not mean it cannot be tried to automatically infer complexities. One tool
% that is developed at the University of Innsbruck which does exactly this is
% \tct~\cite{avanzini2016tct}. On the other hand this negative result also means that
Nonetheless, the undecidability result~\cite{landi1992undecidability} proves that some sort of
creativity, regardless whether of human or artificial origin, is needed to be able to successfully
execute static analysis on a broad variety of input programs. This creativity is usually refereed to
as a \textit{strategy}, which is used to analyse the computer program. In \tct{} this is implemented
by specifying a tree or simultaneously executed forest of sequences of the considered methods in
combination with the fraction of time given for each sub-strategy. However, as a successful analysis
of a custom input program correlates with the specification of applied strategies it is difficult to
build a compound strategy at compile time.

The research area of reinforcement learning tries to mimic human
behavior~\cite{schneckenreither2020average}, and thus in some sense also creativity. Additionally,
over the past years vast progress was reported in reinforcement learning (RL), which has lead to
astonishing results in the application of its algorithms to game-like domains, for instance
chess~\cite{Silver17_MasteringChessAndShogiBySelfPlayWithAGeneralReinforcementLearningAlgorithm},
Go~\cite{Silver16_MasteringTheGameOfGoWithDeepNeuralNetworksAndTreeSearch}, and various Atari
games~\cite{Mnih15_HumanlevelControlThroughDeepReinforcementLearningb}. RL is an optimisation
technique that stems from dynamic programming and tries to find the best stationary policy for a
given problem~\cite{sutton1998introduction}. This policy is usually provided by assessing the states
(or state-action pairs) of an underlying Markov Decision Process (MDP). The advantage of RL over
dynamic programming is that (i) the problem space is explored by an agent and thus only expectantly
interesting parts of the problem space need to be assessed and (ii) the knowledge (acquisition) of
transition probabilities becomes unnecessary as the states are evaluated by consecutively observed
states solely~\cite{sutton1998introduction}. Recently average reward adjusted discounted RL has been
shown to decrease the number of learning steps, further it is capable of inferring better policies
and in contrast to standard discounted RL can also be applied to tasks imposing a non-zero average
reward per step taken~\cite{schneckenreither2020average}. The latter property enables specifying
intermediate rewards and thus a more natural learning process, similar like humans reward their
children with compliments for making progress in a board game, although the game has not come to an
end.

These enhancements in RL in combination with the fact that learned policies can easily be stored,
improved over time and a lookup of their knowledge is immediate put themselves forward to be
utilised as appropriate creativity mechanism in static code analysers.
%
Therefore, we propose to integrate reinforcement learning to introduce the required creativity for
dynamic strategy building in \tct{}. This allows to build specialised strategies according to the
programs input characteristics.
%
In the offline learning phase the agent accumulates knowledge on how to build strategies by
executing a wide variety of input programs with different sequences of methods. It does so by
mapping the program characteristics, such as number of rules, root symbols, strongly connected
components, etc\@. to actions like adding polynomial interpretations, matrix interpretations, or
other methods to the current strategy sequence. This is done by assigning a value to each possible
action according to the current strategy on hand. These valuations arise from reporting the
complexity bound and time needed, whereas the reinforcement learning agent connects consecutive
states accordingly~\cite{sutton1998introduction}.
%
Then, when \tct{} is executed, this knowledge, saved as an artificial neural network, is used to
build a corresponding strategy that fits the input program. The artificial neural network ensures
that the agent can generalise and thus also interpolate in case of unfamiliar input features.

%
% Furthermore, it can improve its knowledge after the initial learning phase, which ensures that it
% can learn how to build strategies from inputs specified at the official \tct{} web
% interface\footnote{See http://colo6-c703.uibk.ac.at/tct/index.php}.

% This
% clearly machine learning in program analysis, namely to introduce some kind of creativity to be able
% to improve the number of successfully analysed input programs.

% Related lit
Using machine learning for selecting strategies in static program analysis is no novelty. For
instance, Demyanova et al.~\cite{demyanova2017empirical} identify program metrics, such as loop
patterns and control-flow complexity indicators, to build a machine-learning based solver for
software verification. Similarly, but in a different research field, Barstad et
al.~\cite{barstad2014predicting} predict code quality using machine learning. However, to the best
of your knowledge there is no work that combines reinforcement learning with space and runtime
complexity analysers to reduce the issues inevitable introduced through undecidability.

% infer (asymptotic) space and runtime of

% \MS[t]{todo paper} show that by combining \MS[t]{...} \cite{demyanova2017empirical}.


% \tct{} includes several different complexity methods, which can be used to infer complexity bounds
% of TRS. The modularity of \tct{}s implementation allows the application of the techniques in a
% sequence and thus the process of finding the complexity of the input program is divided into several
% sub-problems \cite{avanzini2016tct}. The frameworks' complexity techniques are modelled as
% complexity processors, which are applied according to some strategy. For defining strategies \tct{}
% uses a powerful strategy language. It turns a set of processors into a sophisticated complexity
% analyser \cite{avanzini2016tct}. Today this strategy is hard-coded in the code and does not adapt
% automatically according to the input problem.


% In this thesis we will improve \tct{} in two ways. First we will add an additional complexity
% processor which implements amortised resource analysis for term rewrite systems. This processor will
% allow both, worst-case runtime complexity and best-case runtime complexity analyses of computer
% programs. And second we will use machine learning to build an adaptive strategies according to the
% input program the user specifies. This will improve the overall number of solved instances for
% analyses over different types of programs and also will reduce the manual interaction needed when
% analysing programs for their complexity. The aim is to use reinforcement learning to constantly be
% able to adapt the strategy to the type of input programs. The learner should automatically learn how
% to build strategies based on characteristics of the input by using a given testbed. Furthermore, it
% should be able to improve its knowledge after the initial learning phase. This ensures that it can
% learn how to build strategies from inputs specified at the official \tct{} web
% interface\footnote{See http://colo6-c703.uibk.ac.at/tct/index.php}.

The rest of this paper is structured as follows. The next section introduces average reward adjusted
RL\@. Section~\ref{sec:dyn} illustrates the schematic setup for building dynamic strategies in
\tct{} and Section~\ref{sec:conclusion} concludes the paper.


\section{Average Reward Adjusted Reinforcement Learning}\label{sec:average}

This section introduces the basic ideas of standard and average reward adjusted discounted
reinforcement learning and is largely based on the ideas presented
in~\cite{schneckenreither2020average}.

The vast majority of methods used in RL can split into three categories~\cite{konda2000actor}: 1.\@
Actor-only (policy iteration) methods, which improve the learned policy directly, 2.\@ Critic-only
(value iteration) methods which exclusively learn a state value function that is then used to infer
a policy, thus this is an indirect approach, and 3.\@ Actor-Critic methods which learn both a policy
and a value function, whereas the value function update is used as a measure for the policy
parameters update. This overcomes the drawbacks of actor-only methods in which each update is
independent of previous estimates and the approximate solutions plus some arising issues like
catastrophic forgetting~\cite{french1999catastrophic} of critic-only approaches.

Discounted RL, both the standard and average reward adjusted approach, are based on value-iteration.
Thus they seeks to approximate a value function that describes the reward received when the agent
navigates through the solution space. Therefore this paper concentrates on the critic-only method,
whereas an implementation of the corresponding actor-critic approach is straightforward.

\paragraph{Discounted Reinforcement Learning.}
In standard discounted RL the aim is to approximate the expected discounted sum of future rewards
\(V_{\gamma}^{\pol}(s)\). Formally it is defined under a stationary policy \(\pol\) and for any
state \(s\) by
\begin{align*}
  V_{\gamma}^{\pol}(s) \defsym \lim_{N \to \infty} E[\sum_{t=0}^{N-1} \gamma^{t} R_{t}^{\pol}(s)]\tcom
\end{align*}
where \(0 \leqslant \gamma < 1\) is the discount factor and
\(R_{t}^{\pol}(s) = \E_{\pol}[ r(s_{t},a_{t}) \mid s_{t} = s, a_{t} = a]\) the reward received at
time \(t\) upon starting in state \(s\) by following policy
\(\pol\)~\cite[e.g.]{Mahadevan96_AverageRewardReinforcementLearningFoundationsAlgorithmsAndEmpiricalResults}.
%
% The aim is to find an optimal policy \(\polopt_{\gamma}\), which when followed, maximises the state
% value for all states \(s\) as compared to any other policy \(\pol_{\gamma}\):
% $V_{\gamma}^{\polopt_{\gamma}} - V_{\gamma}^{\pol_{\gamma}} \geqslant 0\tpkt$
Due to Miller and Veinott~\cite{MillerVeinott1969} the expected discount sum of rewards can be
decomposed using the Laurent series expansion yielding
%
% \begin{align*}
%   \label{eq:laurent}
  \(V_{\gamma}^{\pol}(s) = \frac{\avgrew^{\pol}(s)}{1-\gamma} + V^{\pol}(s) + e_{\gamma}^{\pol}(s)\),
% \end{align*}
%
  where \(\avgrew^{\pol}(s)\) is the average reward per step received, \(V^{\pol}(s)\) the
  additional reward that sums up when the process starts in state \(s\), usually referred to as bias
  value, and \(e_{\gamma}^{\pol}(s)\) an error term for which Puterman~\cite[p.341]{Puterman94}
  shows that \(\lim_{\gamma \to 1} e_{\gamma}^{\pol}(s) = 0\), and thus it only exists as \(\gamma\)
  is strictly less than \(1\).

\paragraph{Average Reward Adjusted Reinforcement Learning.} In contrast to discounted RL, average
reward adjusted RL focuses on learning the average reward \(\avgrew^{\pol}(s)\) and the bias value
\(V^{\pol}(s)\) separately.
%
The \textit{average reward} \(\avgrew^{\pol}(s)\) of a policy \(\pol\) and a starting state \(s\)
was first defined by Howard~\cite{howard1960dynamic} as
%
\begin{align*}
  \avgrew^{\pol}(s) \defsym \lim_{N \to \infty} \frac{\E [\sum_{t=0}^{N-1}R_{t}^{\pol}(s)]}{N}\tcom
\end{align*}
%
where as above \(R_{t}^{\pol}(s)\) is the reward received at time \(t\), starting in state \(s\) and
following policy \(\pol\). In the common case of unichain (and thus ergodic) MDPs, in which only a
single set of recurrent states exists, the average reward \(\avgrew^{\pol}(s)\) is equal for all
states
\(s\)~\cite{Mahadevan96_AverageRewardReinforcementLearningFoundationsAlgorithmsAndEmpiricalResults,Puterman94}
and therefore in such cases may simply be referred to as \(\avgrew^{\pol}\). Then the \textit{bias
  value} is defined as
%
\begin{align*}
  V^{\pol}(s) \defsym \lim_{N \to \infty}{ \E [ \sum_{t=0}^{N-1}(R_{t}^{\pol}(s) - \avgrew^{\pol}(s) )]}\tcom
\end{align*}
%
where again \(R_{t}^{\pol}(s)\) is the reward received at time \(t\), starting in state \(s\) and
following policy \(\pol\). Finally, we define the \textit{average reward adjusted discounted state
  value} $\X_{\gamma}^{\pol}(s)$ of a state $s$ under policy $\pol$ and with discount factor
$0 \leqslant \gamma \leqslant 1$ as
%
\begin{align*}
  \X_{\gamma}^{\pol}(s) \defsym V^{\pol}(s) + e_{\gamma}^{\pol}(s)\tpkt
\end{align*}
%
Note that by decomposing the state values we are able to set \(\gamma\) arbitrary close to \(1\),
including \(1\). For simplicity in the rest of this paper we set \(\gamma = 1\) and therefore
\(\X_{1}^{\pol}(s) = \X^{\pol}(s)\) = V$^{\pol}(s)$. Model-free reinforcement learning assesses
state-action \((s,a)\) tuples instead of states \(s\) solely. Although this increases the number of
states it allows learning without providing any model information.

Algorithm~\ref{alg:near} presents a simplified version of adjusted average reward RL, similar to the
one presented by Schwartz in~\cite{schwartz1993reinforcement}. For the comprehensive version
see~\cite{schneckenreither2020average}, where however the objective is flipped from minimisation to
maximisation of rewards. In line 6 the average reward is updated in a exponentially smoothed manner
by the current estimates of the consecutive average reward adjusted discounted state values in case
a non-random action was taken. The formula used is a reformulation of the second addend of the
Laurent series expansion~(see~\cite{MillerVeinott1969} or~\cite[p.346]{Puterman94} for details):
\(\avgrew^{\pol}(s) + V^{\pol}(s) - E[V^{\pol}(s)] = R_{t}(s)\). Then in line 7 the state values of
the state action tuple \((s_{t}, a_{t})\) is updated using an estimate of the observed consecutive
state, plus the immediate and average reward.

Therefore, average reward adjusted RL decomposes the discounted state values into its sub-components
and learns these components separately. This has the advantages of i) being able to find more
optimal policies, ii) reduces the number of steps in the iterative learning process, iii) prevents
cluster formations as the average reward is not learned independently for every state and finally iv)
eases the representation for the approximation of the state function as the state-values are spread
further apart~\cite{schneckenreither2020average}. However, recall the assumption of unichain MDPs,
i.e.\@ there must not exist a state which splits the possible consecutive states 

\begin{algorithm}[t!]
  \begin{algorithmic}[1]
    \State{}Initialize state \(s_{0}$, $\avgrew^{\pol} = 0\), \(\X^{\pol}(\cdot, \cdot) = 0\), set
    an exploration rate \(0 < p_{exp} \leqslant 1\) and learning rates \(0 < \alpha, \gamma < 1\).
    \While{the stopping criterion is not fulfilled}
    \State{}
    \begin{minipage}[t]{0.9\textwidth} With probability \(p_{exp}\) choose a random action
      and probability \(1-p_{exp}\) one with \(\min_{a} \X^{\pol}(s_{t},a)\).
    \end{minipage}
    \State{}Carry out action \(a_{t}\), observe reward \(r_{t}\) and resulting state \(s_{t+1}\).
    \If{a non-random action was chosen}
    \State{}
    \begin{minipage}{0.9\textwidth}
      \centering
      \(\avgrew^{\pol}  \gets (1- \alpha) \avgrew^{\pol} + \alpha [r_{t} + \min_{a}\X^\pol(s_{t+1},a) - \X^\pol(s_{t},a_{t})]\)
    \end{minipage}
    \EndIf
    \State{}Update the average reward adjusted discounted state-value.
    \State{}
    \begin{minipage}{0.9\textwidth}
      \centering
      \(\X^\pol(s_{t},a_{t}) \gets (1-\gamma) \X^\pol(s_{t},a_{t}) + \gamma [r_{t} + \min_{a} \X^\pol(s_{t+1},a) - \avgrew^{\pol}]\)
    \end{minipage}

    \State{}Set \(s \gets s'\), \(t \gets t+1\) and decay parameters
    \EndWhile{}
  \end{algorithmic}
  \caption{\label{alg:near}Simplified model-free tabular average reward adjusted RL algorithm for unichain MDPs.}
\end{algorithm}


\section{Dynamic Strategy Building}\label{sec:dyn}

This section introduces how strategies are learned to be build using (average reward adjusted)
discounted reinforcement learning. Thus, we define the state space, the possible actions the agent
can choose, the reward function and the approaches we use to tackle the task. To simplify the
problem, for now we concentrate on polynomial complexity bounds with a maximum degree of
\(P_{\max}\) only. Further we restrict to sequencing of strategies (e.g.\@ strategy \(s\) followed
by \(g\), with operator \(s \seq g\)) and adding them as an alternative with operator \(s \alt g\),
where \tct{} uses strategy \(s\) if it succeeds or \(g\) otherwise. Thus, the agent can build a
binary tree with each leaf containing a sequence of strategies. % If one of the strategies succeeds in
% time, then \tct{} is able to successfully analyse the input.

\begin{figure*}[t!]
  \centering
  \input{mdpschema}             % Figure production system
  \caption{\label{fig:schema}Schema of the MDP for a average reward adjusted RL agent.} 
\end{figure*}

\paragraph{Action Space.} The modularity of \tct{} allows a composition of various strategies with
different combinators. These strategies can be separated into two main types in terms of their
behavior. First decomposition methods~\cite{avanzini2013combination} are used to decompose the input
problem into one or more sub-problems, and second complexity analysers which infer complexity bounds
of the generated sub-problems. Therefore, we will use two structural identical consecutive agents.
First the decompose agent optimise the decompose methods, before another agent builds a complexity
analyser strategy for each sub-problem by combining methods like polynomial
interpretations~\cite{hirokawa2008automated}, matrix interpretations~\cite{moser2008complexity} or
amortised resource analyses~\cite{moser2020automated}. Figure~\ref{fig:schema} depicts the basic
schema of the underlying MDP\@. The idea is that starting with the strategy \textsf{identity} the
agent can choose between sequencing another method using action \(\Seq\), creating an alternative
strategy using action \(\alt\), and returning the current strategy tree using action \(\End\).
%
%
In case of choosing \(\Seq\) the agent chooses one concrete method out of the set of available
methods (e.g.\@ one out of all decomposition methods), which is then sequenced to the current
strategy \(s\) yielding a new strategy \(s \seq t\). During training this new strategy is then
evaluated and a information of the progress is returned as reward, i.e.\@ the agents choice is
assessed. Note that the dashed nodes have been introduced to increase readability only and actually
do not exist. In case the agent ends the process an assessment of the whole strategy is provided to
the agent. The progress reward enables the agent to steadily increase the strategy without the need
of groping in the dark. In the presented schema we assume right-associativity of \(\alt\), leading
to the fact the strategy of Figure~\ref{fig:binary} can only be build in the order provided by the
red numbers. Further, this restricts to binary trees with nodes composed of the alternative-operator
and leaves of sequences of strategies, thus it prevents ambiguity. To avoid an infinite composition
of strategies we set a maximum number of possible iterations.

\begin{figure*}[t!]
  \centering
  \input{binarytree}             % Figure production system
  \caption{\label{fig:binary}Binary tree that can be built only in the order given as red numbers.}
\end{figure*}

\paragraph{State Space.} The state space consists of two types of data. The first part are
characteristics that describe the input problem. This includes, but is not limited to, number of
rules, number of (root) symbols, maximum function arity, number of strongly connected components,
left-linearity, right-linearity, flags specifying whether it is a constructor TRS, orthogonality,
completely defined, etc. The second data input describes the current strategy for the underlying
sub-problem. This can be done using one natural number per possible method standing for the number
of sequence operators since the last occurrence last occurrence of that technique.

\paragraph{Reward Function.} As illustrated in Figure~\ref{fig:schema} the reward function is
composed of two different reward generation actions, once a new method is added and when the
strategy is completed. % The progress of the decomposition agent is reward by
For succinctness we focus on the worst-case time complexity analysing agent. Whenever a new analyser
technique is added or at the end of the strategy building process reached the complexity analysis is
run (in parallel) with the strategy on hand. The result is then encoded as follows, where we assume
a timeout of \(T\) seconds, a measured execution time of \(t\) seconds and resulting complexity \(C\):
\begin{align*}
  r(s_{t}) =
  \begin{cases}
     c T + t    & \qquad   \text{if $C$ is }\bO(n^{c}) \text{ and } c < P_{\max}\\
     P_{\max} T  & \qquad  \text{otherwise}\tpkt
  \end{cases}
\end{align*}
Thus, the reward function produces smaller reward for better complexity bounds and therefore the
objective of the agent is to minimise the accumulated reward. Note that although average adjusted
reinforcement learning is designed for unichain MDPs, i.e.\@ estimates only a scalar average reward,
we are still able to use this technique, as the state-values will adapt appropriately, cf.\@ the
Laurent Series Expansion in Section~\ref{sec:average}. Additionally, the task is episodic and thus
there exists a natural upper bound for the state values.

\paragraph{Approach.} We plan to evaluate both, standard discounted RL, and average reward adjusted
RL\@. For the former, the progress reward is likely to cause issues. Therefore, it might be the case
that the MDP is adapted by dropping the appropriate early feedback. Additionally, we will lift the
presented critic-only method of Section~\ref{sec:average} to an asynchronous actor-critic method, as
done in~\cite{mnih2016asynchronous}. 


% \tct{} defines 

% introduces the possible actions


% \begin{itemize}
% \item Action Space
% \item State Space
%   \begin{itemize}
%   \item Input Program Characteristics
%   \item Current Strategy 
%   \end{itemize}
% \item Reward function  
% \item Unichain problematic
% \item Running Time (parallel execution, and using a DB up to a degree of strategy depth)
% \item Approaches: DQN. Adjusted Average Reward DQN, Actor-Critic
% \end{itemize}

\section{Conclusion}\label{sec:conclusion}

In this paper we present an schematic approach on how to use reinforcement learning, with a special
focus on average reward adjusted reinforcement learning to generate dynamic strategies in \tct{}. We
plan to implement the proposed method and evaluate the result in comparison to current human-made
strategies and among different reinforcement learning agents.

% optimise strategy 

% In conclusion we plan to investigate both, standard discounted reinforcement learn

% \MS[t]{TODO}
% In this thesis we will improve the performance of the static program analyser \tct{} in two ways.
% First we will integrate amortised analysis for worst-case runtime and best-case complexity. This
% enables another complexity processor in \tct{} strategy building. Then we concentrate on dynamic
% strategy building for \tct{}, such that the input program characteristics are used to build a
% specific strategy for the given input program. By using dynamic strategies we expect to be able to
% increase the performance of \tct{} and provide a fully automatic tool for program complexity
% analyses, with no need for user configuration.


\bibliographystyle{plain}
\bibliography{references}


\appendix
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
